{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs7HDv-RIYri"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Addn_9afRq61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "avU4qaHlRt_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing"
      ],
      "metadata": {
        "id": "Mp8VM79EIlIm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbQURVnYIl5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-1. Take any YouTube videos link and your task is to extract the comments from\n",
        "that videos and store it in a csv file and then you need define what is most\n",
        "demanding topic in that videos comment section.\n"
      ],
      "metadata": {
        "id": "yixncIh0Ivaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-api-python-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJjzWRz_KZcr",
        "outputId": "e9ee2a73-683c-44ed-a407-885e71d24211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.21.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.59.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import csv"
      ],
      "metadata": {
        "id": "n6pMH-lmIxEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the YouTube Data API client\n",
        "api_key = 'AIzaSyCG8P3E01JaB-rigIabHmuxnPrS-A-Kxuo'\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "video_id = 'Gs15V79cauo'# Replace with the actual video ID or URL\n",
        "comments = []\n",
        "next_page_token = None\n",
        "\n",
        "while True:\n",
        "    response = youtube.commentThreads().list(part='snippet',videoId=video_id,pageToken=next_page_token,maxResults=100).execute()\n",
        "\n",
        "    for item in response['items']:\n",
        "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "        comments.append(comment)\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "lvEY6C42I7T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = 'comments.csv'\n",
        "\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Comment'])\n",
        "    writer.writerows([[comment] for comment in comments])\n",
        "\n",
        "print('Comments extracted and saved to', csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH8__q9CKB9b",
        "outputId": "e6c401b5-2d1b-40dd-b0be-4d1d8cbeebbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comments extracted and saved to comments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and tokenize comments\n",
        "import csv\n",
        "from collections import Counter\n",
        "processed_comments = []\n",
        "stopwords = set(['a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', 'was', 'were'])  # add more stopwords if needed\n",
        "\n",
        "for comment in comments:\n",
        "    # Preprocess comment text (remove unwanted characters, punctuation, etc.)\n",
        "    # Tokenize the preprocessed comment into words or phrases\n",
        "    tokens = comment.lower().split()\n",
        "    processed_tokens = [token for token in tokens if token not in stopwords]\n",
        "    processed_comments.extend(processed_tokens)\n",
        "\n",
        "# Count word frequencies\n",
        "\n",
        "word_frequencies = Counter(processed_comments)\n",
        "\n",
        "# Sort word frequencies in descending order\n",
        "\n",
        "sorted_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract most demanding topic\n",
        "\n",
        "most_demanding_topic = sorted_frequencies[0][0]\n",
        "\n",
        "# Write comments and frequencies to a CSV file\n",
        "\n",
        "csv_filename = 'comments.csv'\n",
        "\n",
        "with open(csv_filename, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Comment', 'Frequency'])\n",
        "    writer.writerows(sorted_frequencies)\n",
        "\n",
        "# Print the most demanding topic\n",
        "\n",
        "print(f'The most demanding topic in the comment section is: {most_demanding_topic}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt1VGtKGKDD8",
        "outputId": "c969722d-504d-466a-be69-9b3406a2c189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most demanding topic in the comment section is: you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m41MWKBTHXxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2. Take any pdf and your task is to extract the text from that pdf and store it in a\n",
        "csv file and then you need to find the most repeated word in that pdf."
      ],
      "metadata": {
        "id": "og9vGIY5Het5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install libraries"
      ],
      "metadata": {
        "id": "b2WLtMteHfjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UPVWFzEVn93",
        "outputId": "cef76dbd-9c0b-403c-fd3c-93faac7aa000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import csv"
      ],
      "metadata": {
        "id": "0tz0xTOSVrwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the PDF file and extract text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"/content/(ASCE)GT.1943-5606.0001990.pdf\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
        "        num_pages = pdf_reader.numPages\n",
        "\n",
        "        for page_number in range(num_pages):\n",
        "            page = pdf_reader.getPage(page_number)\n",
        "            text += page.extractText()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "FOrjkC6EV-Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to preprocess the extracted text"
      ],
      "metadata": {
        "id": "1sB2GeMVWpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "6Uw833gVWy_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the most repeated word\n",
        "def find_most_repeated_word(tokens):\n",
        "    word_counts = Counter(tokens)\n",
        "    most_common_word = word_counts.most_common(1)[0]\n",
        "    return most_common_word"
      ],
      "metadata": {
        "id": "Otf3bjDDW2d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(pdf_path, csv_path):\n",
        "    # Extract text from PDF\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Preprocess the extracted text\n",
        "    tokens = preprocess_text(text)\n",
        "\n",
        "    # Find the most repeated word\n",
        "    most_repeated_word = find_most_repeated_word(tokens)\n",
        "\n",
        "    # Store results in a CSV file\n",
        "    with open(csv_path, \"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Word\", \"Count\"])\n",
        "        writer.writerow(most_repeated_word)\n",
        "\n",
        "    print(\"Extraction and analysis complete!\")\n",
        "\n",
        "# Provide the path to your PDF file and the desired CSV file path\n",
        "pdf_path = \"path/to/your/pdf/file.pdf\"\n",
        "csv_path = \"path/to/your/csv/file.csv\"\n",
        "\n",
        "# Call the main function\n",
        "process_pdf(pdf_path, csv_path)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "6AloJE10XNsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "e7169968-e46e-4f13-85d3-90f3e1193e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2a4744940cde>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Call the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprocess_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2a4744940cde>\u001b[0m in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path, csv_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Extract text from PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Preprocess the extracted text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-20e755faa2e2>\u001b[0m in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/(ASCE)GT.1943-5606.0001990.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnum_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumPages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/pdf/file.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYQzb2X1RvUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-4. Take any text file and now your task is to Text Summarization without using\n",
        "hugging transformer library"
      ],
      "metadata": {
        "id": "jjqI4KNaTrlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPxXdt3WV2vN",
        "outputId": "b0e1a26f-a178-4dfb-9c67-da965a514e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file and read its content\n",
        "with open('/content/new 5.txt', 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "0bhZoY8WV8ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_6vmZiTQV0RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "zgevMD8hYDu0",
        "outputId": "0c159ad6-8244-40ed-a0c4-a2bf3b9e213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A file with .TXT extension represents a text document that contains plain text in the form of lines. Paragraphs in a text document are recognized by carriage returns and are used for better arrangement of file contents. A standard text document can be opened in any text editor or word processing application on different operating systems. All the text contained in such a file is in human-readable format and represented by sequence of characters.\\n\\nText files can store large amount of data as there is no limitation on the size of contents. However, text editors opening such large files need to be smart for loading and displaying these. Almost all operating systems come with text editors that allow you to create and edit text files. For example, Windows OS comes with Notepad and Wordpad for this purpose. Similarly, MacOS comes with TextEdit for creating and editing Text Documents. There are, however, other free text editors available as well over the internet that provide you the capability to work with Text Documents like Notepad++ which is far more advanced in terms of functionality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the text into individual sentences\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ExFHws1YEXN",
        "outputId": "70367071-ec80-4902-a509-2db06cfdb316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A file with .TXT extension represents a text document that contains plain text in the form of lines.',\n",
              " 'Paragraphs in a text document are recognized by carriage returns and are used for better arrangement of file contents.',\n",
              " 'A standard text document can be opened in any text editor or word processing application on different operating systems.',\n",
              " 'All the text contained in such a file is in human-readable format and represented by sequence of characters.',\n",
              " 'Text files can store large amount of data as there is no limitation on the size of contents.',\n",
              " 'However, text editors opening such large files need to be smart for loading and displaying these.',\n",
              " 'Almost all operating systems come with text editors that allow you to create and edit text files.',\n",
              " 'For example, Windows OS comes with Notepad and Wordpad for this purpose.',\n",
              " 'Similarly, MacOS comes with TextEdit for creating and editing Text Documents.',\n",
              " 'There are, however, other free text editors available as well over the internet that provide you the capability to work with Text Documents like Notepad++ which is far more advanced in terms of functionality.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the sentences by removing any unnecessary characters or stopwords.\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_frequencies = {}\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    words = [word for word in words if word not in stop_words and word not in punctuation]\n",
        "    for word in words:\n",
        "        if word not in word_frequencies:\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS-rjpYJYRUc",
        "outputId": "148de5ec-cbf9-43b4-e09b-411bb1a53ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate a relevance score for each sentence based on its importance within the text. This score can be determined by various methods, such as word frequency, sentence length, or other heuristics.\n",
        "sentence_scores = {}\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    sentence_score = 0\n",
        "    for word in words:\n",
        "        if word in word_frequencies:\n",
        "            sentence_score += word_frequencies[word]\n",
        "    sentence_scores[sentence] = sentence_score"
      ],
      "metadata": {
        "id": "P57x2MpiYpVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the top N sentences with the highest relevance scores to form the summary.\n",
        "\n",
        "summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get)"
      ],
      "metadata": {
        "id": "mF7NMSJiZ8K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine the selected sentences and generate the final summary.\n",
        "summary=''.join(summary_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQkG6jlsaXKU",
        "outputId": "3c8cb772-1fc7-4de6-c615-421de42fdaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are, however, other free text editors available as well over the internet that provide you the capability to work with Text Documents like Notepad++ which is far more advanced in terms of functionality.Almost all operating systems come with text editors that allow you to create and edit text files.A standard text document can be opened in any text editor or word processing application on different operating systems.A file with .TXT extension represents a text document that contains plain text in the form of lines.Paragraphs in a text document are recognized by carriage returns and are used for better arrangement of file contents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE0m1IBma7_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5. Now you need build your own language detection with the fast Text model\n",
        "by Facebook"
      ],
      "metadata": {
        "id": "USE-mCLzegCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF17Ky_oegtl",
        "outputId": "8ccaff93-ded9-4375-d177-8aab4e2abab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the training data\n",
        "import fasttext\n",
        "\n",
        "# Set the training parameters\n",
        "lr = 0.1  # learning rate\n",
        "dim = 100  # size of word vectors\n",
        "epoch = 25  # number of training epochs\n",
        "\n",
        "# Train the language detection model\n",
        "model = fasttext.train_supervised(input=\"/content/new 5.txt\", lr=lr, dim=dim, epoch=epoch)"
      ],
      "metadata": {
        "id": "5Xx2sx-njPBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "result = model.test(\"test_data.txt\")\n",
        "precision = result[1]  # Precision is at index 1\n",
        "recall = result[2]  # Recall is at index 2\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYR2yF2jZuw",
        "outputId": "10a68776-1f5d-441d-ae0a-b11c99c598d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: nan\n",
            "Recall: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the language of a text sample\n",
        "text = \"This is a test sentence.\"\n",
        "predicted_label, _ = model.predict(text)\n",
        "print(f\"Predicted Language: {predicted_label[0].replace('__label__', '')}\")\n"
      ],
      "metadata": {
        "id": "WwO11f-NkGQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}